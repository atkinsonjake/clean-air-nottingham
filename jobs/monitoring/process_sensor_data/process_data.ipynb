{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'processed_data_multichannel.json'\n",
    "with open(path, 'r') as file:\n",
    "    sensor_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "\n",
    "# Iterate over the key, value pairs in the dictionary\n",
    "for sensor_id, readings in sensor_data.items():\n",
    "    # Iterate over the list of dictionaries for each sensor_id\n",
    "    for reading in readings:  # Assuming there is only one list per sensor_id\n",
    "        # Add the sensor_id to the dictionary\n",
    "        reading['sensor_id'] = sensor_id\n",
    "        # Append the dictionary to the flattened_data list\n",
    "        flattened_data.append(reading)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = datetime.datetime.utcfromtimestamp(df.time_stamp.min())\n",
    "max_time = datetime.datetime.utcfromtimestamp(df.time_stamp.max())\n",
    "\n",
    "print(min_time.strftime(\"%d/%m/%y\"), max_time.strftime(\"%d/%m/%y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_channels(dataframe, channel_a, channel_b, max_diff=5, max_pct_diff=0.61) -> pd.DataFrame:\n",
    "    '''\n",
    "    Filters rows in the DataFrame based on the absolute and percentage difference \n",
    "    between two specified columns representing sensor channels A and B.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): A pandas DataFrame with air pollution data.\n",
    "        channel_a (str): The name of the first channel (e.g., 'pm1.0_atm_a').\n",
    "        channel_b (str): The name of the second channel (e.g., 'pm1.0_atm_b').\n",
    "        max_diff (float, optional): The maximum acceptable absolute difference \n",
    "                                    in Î¼g between the two channels. Default is 5.\n",
    "        max_pct_diff (float, optional): The maximum acceptable percentage difference \n",
    "                                        between the two channels, expressed as a fraction \n",
    "                                        (e.g., 0.61 for 61%). Default is 0.61.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame filtered based on the specified criteria.\n",
    "    '''\n",
    "    # Abs difference\n",
    "    absolute_difference = abs(dataframe[channel_a] - dataframe[channel_b])\n",
    "    # Pc difference, where the denominator is not zero to avoid division by zero\n",
    "    percentage_difference = abs((dataframe[channel_a] - dataframe[channel_b]) / dataframe[channel_b].replace(0, float('nan')))\n",
    "    \n",
    "    condition = (absolute_difference <= max_diff) & (percentage_difference <= max_pct_diff)\n",
    "    return dataframe[condition].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate differences\n",
    "def calculate_differences(row, channel_a, channel_b, max_diff, max_pct_diff):\n",
    "    # Compute the absolute difference\n",
    "    absolute_difference = abs(row[channel_a] - row[channel_b])\n",
    "    # Compute the percentage difference, handling the division by zero\n",
    "    percentage_difference = abs((row[channel_a] - row[channel_b]) / row[channel_b] if row[channel_b] != 0 else float('nan'))\n",
    "\n",
    "    # Return the differences\n",
    "    return pd.Series([absolute_difference, percentage_difference], index=['abs_diff', 'pct_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['abs_diff', 'pct_diff']] = df.apply(lambda row: calculate_differences(row, 'pm2.5_atm_a', 'pm2.5_atm_b', 5, 0.61), axis=1)\n",
    "condition = (df['abs_diff'] <= 5) & (df['pct_diff'] <= 0.61)\n",
    "filtered_df = df[condition]\n",
    "filtered_df = filtered_df.drop(columns=['abs_diff', 'pct_diff'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.rename(columns={\"time_stamp\":\"unixtime\"})\n",
    "data = filtered_df.copy()\n",
    "data['datetime'] = pd.to_datetime(data['unixtime'], unit='s')\n",
    "\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['time'] = data['datetime'].dt.time\n",
    "data['day_of_week'] = data['datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.day.min(), data.day.max(), data.month.min(), data.month.max(), data.year.min(), data.year.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.day_of_week.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pm1.0_atm'] = data[['pm1.0_atm_a', 'pm1.0_atm_b']].mean(axis=1)\n",
    "data['pm2.5_atm'] = data[['pm2.5_atm_a', 'pm2.5_atm_b']].mean(axis=1)\n",
    "data['pm10.0_atm'] = data[['pm10.0_atm_a', 'pm10.0_atm_b']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_select = data[['sensor_id', 'humidity', 'temperature', 'pressure', 'datetime', 'day', 'month', 'year', 'time',\n",
    "       'day_of_week', 'pm1.0_atm', 'pm2.5_atm', 'pm10.0_atm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars = ['humidity', 'temperature', 'pressure', 'pm1.0_atm', 'pm2.5_atm', 'pm10.0_atm']\n",
    "\n",
    "# Melt the DataFrame\n",
    "melted_data = data_select.melt(id_vars=['sensor_id', 'datetime', 'day', 'month', 'year', 'time', 'day_of_week'],\n",
    "                               value_vars=value_vars,\n",
    "                               var_name='field',\n",
    "                               value_name='reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_data.day_of_week.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings = melted_data.groupby(['sensor_id', 'field', 'day_of_week', 'time']).resample('H', on='datetime')['reading'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_hourly_readings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings.datetime.min(), data_hourly_readings.datetime.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings.day_of_week.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings.to_csv('processed_data_can_sensors_hourly_calibrated_wide.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_wide = data_hourly_readings.pivot(index=['sensor_id', 'datetime'], columns='field', values='reading').reset_index()\n",
    "data_hourly_readings_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_wide.to_csv('processed_data_can_sensors_hourly_calibrated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_reading_generic(s1: float, s2: float, s3: float, pm25: float, rh: float, i: float) -> float:\n",
    "    \"\"\"\n",
    "    Calibrates a PM2.5 reading.\n",
    "\n",
    "    Args:\n",
    "        s1: the first coefficient in the calibration equation.\n",
    "        s2: the second coefficient in the calibration equation.\n",
    "        s3: the third coefficient in the calibration equation.\n",
    "        pm25: the PM2.5 reading in raw units.\n",
    "        rh: the relative humidity in percent.\n",
    "        i: an intercept.\n",
    "\n",
    "    Returns:\n",
    "        The calibrated PM2.5 reading in ug/m^3.\n",
    "    \"\"\"\n",
    "\n",
    "    adjusted_rh = rh**2 / (1 - rh)\n",
    "    return s1 * pm25 + s2 * adjusted_rh * pm25 + s3 * adjusted_rh + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_reading(s1: float, s2: float, pm25: float, rh: float, i: float) -> float:\n",
    "    \"\"\"\n",
    "    Calibrates a PM2.5 reading.\n",
    "\n",
    "    Args:\n",
    "        s1: the first coefficient in the calibration equation.\n",
    "        s2: the second coefficient in the calibration equation.\n",
    "        pm25: the PM2.5 reading in raw units.\n",
    "        rh: the relative humidity in percent.\n",
    "        i: an intercept.\n",
    "\n",
    "    Returns:\n",
    "        The calibrated PM2.5 reading in ug/m^3.\n",
    "    \"\"\"\n",
    "\n",
    "    return s1 * pm25 - s2 * rh + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars=['sensor_index', 'time'], var_name='fields', value_name='reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"time\":\"unixtime\"})\n",
    "data['datetime'] = pd.to_datetime(data['unixtime'], unit='s')\n",
    "\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['time'] = data['datetime'].dt.time\n",
    "data['day_of_week'] = data['datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('processed_data_can_sensors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['datetime'].min())\n",
    "print(data['datetime'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sensor_index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fields.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.fields == 'humidity'].reading.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('sensor_index').fields.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings = data.groupby(['sensor_index', 'fields']).resample('H', on='datetime')['reading'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_wide = data_hourly_readings.pivot(index=['sensor_index', 'datetime'], columns='fields', values='reading').reset_index()\n",
    "data_hourly_readings_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_wide['pm2.5_calibrated'] = data_hourly_readings_wide.apply(lambda x: calibrate_reading(0.524, 0.0862, x['pm2.5_atm'], x['humidity'], 5.75), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_long = data_hourly_readings_wide.melt(id_vars=['sensor_index', 'datetime'], var_name='fields', value_name='reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_long['day'] = data_hourly_readings_long['datetime'].dt.day\n",
    "data_hourly_readings_long['month'] = data_hourly_readings_long['datetime'].dt.month\n",
    "data_hourly_readings_long['year'] = data_hourly_readings_long['datetime'].dt.year\n",
    "data_hourly_readings_long['time'] = data_hourly_readings_long['datetime'].dt.time\n",
    "data_hourly_readings_long['day_of_week'] = data_hourly_readings_long['datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_readings_long.to_csv('processed_data_can_sensors_hourly_calibrated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_hourly_readings_long.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_select = data[data.fields == 'pm1.0_atm']\n",
    "grouped_data = data_select.groupby('sensor_index')\n",
    "\n",
    "# Create a plot for each group\n",
    "for name, group in grouped_data:\n",
    "    plt.plot(group['datetime'], group['reading'], label=name)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Reading')\n",
    "plt.title('Readings over Time')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variables = [\"pm1.0_atm\", \"pm2.5_calibrated\", \"pm10.0_atm\"]\n",
    "groupbys = ['datetime', 'day_of_week', 'month']\n",
    "\n",
    "for grouping in groupbys:\n",
    "    for var in variables:\n",
    "        grouped_data = data.groupby(by=grouping).mean()\n",
    "        sns.lineplot(data=grouped_data, x=grouped_data.index, y=\"reading\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-can",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
